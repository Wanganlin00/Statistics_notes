# 广义线性模型

在统计学上，广义线性模型（generalized linear model， GLM）是一种应用灵活的线性回归模型。该模型允许因变量的误差分布有除了正态分布之外的其它分布。此模型假设实验者所测量的随机变量的分布函数与实验中系统性效应（即非随机的效应）可经由一连接函数（link function）建立可解释其相关性的函数。[wikipedia](https://zh.wikipedia.org/zh-cn/%E5%BB%A3%E7%BE%A9%E7%B7%9A%E6%80%A7%E6%A8%A1%E5%9E%8B)

在广义线性模式中，假设每个资料的观测值 Y 来自某个指数族分布 f 。

## 广义线性模型组件

广义线性模型是对线性模型的扩展，适用于非正态分布的数据，假设观测值之间是独立的，不能处理组内相关性。模型形式为：

$$
g(E(Y))=\mathbf{X} \beta
$$

1.  线性预测子：

$$
\eta = \mathbf{X} \beta
$$

2.  因变量的期望值与线性预测函数的关系：

$$
E(y)=\mu
$$

3.  连接函数 `g(.)` ：

$$
\eta =g(\mu)=g(E(y))
$$

4.  反连接函数`g-1 (.)`：

$$
E(y)=g^{-1}(\eta)
$$

5.  y 的方差：

    $$
    Var(y)=f(\mu)=f(g^{-1}(\mathbf{X}\beta))
    $$

**典型链接函数**

|                                                                       Y的分布                                                                       |                                           名称                                            |                                                                                                                                                                       链接函数                                                                                                                                                                        |                                                                                                                                                                                                       均值函数                                                                                                                                                                                                        |
|:----------------:|:----------------:|:----------------:|:----------------:|
|                                [正态](https://zh.wikipedia.org/wiki/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83 "正态分布")                                |                                           恒等                                            |                                                 Xβ=μ![{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu \\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/63238c06f9c1927aee60b40fec3adccd419cf32a){alt="{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu \\,\\!}"}                                                 |                                                                                 μ=Xβ![{\\displaystyle \\mu =\\mathbf {X} {\\boldsymbol {\\beta }}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/12c514082234f52d09595635789f474de0279b7d){alt="{\\displaystyle \\mu =\\mathbf {X} {\\boldsymbol {\\beta }}\\,\\!}"}                                                                                 |
|                                [指数](https://zh.wikipedia.org/wiki/%E6%8C%87%E6%95%B8%E5%88%86%E4%BD%88 "指数分布")                                |              [倒数](https://zh.wikipedia.org/wiki/%E5%80%92%E6%95%B8 "倒数")              |                                         Xβ=μ^−1^![{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu \^{-1}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/35c753c466b330a78b576fc8727e188962cc604f){alt="{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu ^{-1}\\,\\!}"}                                          |                                                                      μ=(Xβ)^−1^![{\\displaystyle \\mu =(\\mathbf {X} {\\boldsymbol {\\beta }})\^{-1}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/77e75642db84d5f96e6c2ceb8b6c1deec1b41037){alt="{\\displaystyle \\mu =(\\mathbf {X} {\\boldsymbol {\\beta }})^{-1}\\,\\!}"}                                                                       |
|                               [Gamma](https://zh.wikipedia.org/wiki/%E4%BC%BD%E7%8E%9B%E5%88%86%E5%B8%83 "伽玛分布")                                |                                                                                           |                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                       |
|                         [逆高斯](https://zh.wikipedia.org/wiki/%E9%80%86%E9%AB%98%E6%96%AF%E5%88%86%E4%BD%88 "逆高斯分布")                          |                                         二次倒数                                          |                                         Xβ=μ^−2^![{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu \^{-2}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/0a3b87590326202b24e85ce5762989fd34bff8c2){alt="{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\mu ^{-2}\\,\\!}"}                                          |                                                                   μ=(Xβ)^−1/2^![{\\displaystyle \\mu =(\\mathbf {X} {\\boldsymbol {\\beta }})\^{-1/2}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/9f2b2781a377e3d9ed78c1b1e026fda1e8895402){alt="{\\displaystyle \\mu =(\\mathbf {X} {\\boldsymbol {\\beta }})^{-1/2}\\,\\!}"}                                                                    |
|                                [泊松](https://zh.wikipedia.org/wiki/%E6%B3%8A%E6%9D%BE%E5%88%86%E4%BD%88 "泊松分布")                                | [自然对数](https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E5%B0%8D%E6%95%B8 "自然对数") |                                      Xβ=ln⁡(μ)![{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\ln {(\\mu )}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/ef9f78b057c55a36d8b2516ba1f22a64f601fa1e){alt="{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\ln {(\\mu )}\\,\\!}"}                                      |                                                                    μ=exp⁡(Xβ)![{\\displaystyle \\mu =\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b8cdcc2a7f1ac3de2da641254ab17cd120d1ce5e){alt="{\\displaystyle \\mu =\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}\\,\\!}"}                                                                     |
|                         [二项式](https://zh.wikipedia.org/wiki/%E4%BA%8C%E9%A1%B9%E5%BC%8F%E5%88%86%E5%B8%83 "二项式分布")                          |                   [Logit](https://zh.wikipedia.org/wiki/Logit "Logit")                    | Xβ=ln⁡\[μ/(1−μ)\]![{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\ln {\\left({\\frac {\\mu }{1-\\mu }}\\right)}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b1399fce891de947b987e2e8ae8abd942316a681){alt="{\\displaystyle \\mathbf {X} {\\boldsymbol {\\beta }}=\\ln {\\left({\\frac {\\mu }{1-\\mu }}\\right)}\\,\\!}"} | μ=exp⁡(Xβ)/(1+exp⁡(Xβ)![{\\displaystyle \\mu ={\\frac {\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}}{1+\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}}}\\,\\!}](https://wikimedia.org/api/rest_v1/media/math/render/svg/d6ffa52841bbde6699049034181cc032d5f14533){alt="{\\displaystyle \\mu ={\\frac {\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}}{1+\\exp {(\\mathbf {X} {\\boldsymbol {\\beta }})}}}\\,\\!}"} |
| [多项式](https://zh.wikipedia.org/w/index.php?title=%E5%A4%9A%E9%A0%85%E5%BC%8F%E5%88%86%E4%BD%88&action=edit&redlink=1 "多项式分布（页面不存在）") |                                                                                           |                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                       |

```{r}
library(tidymodels)
library(poissonreg)
library(patchwork)
```

## 高斯线性回归

高斯线性回归是一种简单的线性回归模型，其中假设响应变量服从正态分布，并且使用恒等连接函数（identity link function）。t-statistic

```{r}

# 使用 glm() 函数进行高斯线性回归
glm1 <- linear_reg() %>% 
  set_engine("glm", family = stats::gaussian(link = "identity")) %>% 
  fit(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality,
      data = swiss)

# 查看模型的系数
tidy(glm1)

# 查看模型的 AIC 和 Deviance
glance(glm1) %>% dplyr::select(AIC, deviance)
```

## 逻辑回归

逻辑回归用于处理二元分类问题。其模型假设响应变量的对数优势（log odds）服从线性模型。

Sigmoid 激活函数：

$$
f(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{1+e^x}
$$

```{r}
sigmoid <- tibble(
    x=seq(-6,6,length.out=1000),
    y=1/(1+exp(-x)),
)
ggplot(sigmoid,aes(x,y))+
    geom_line()
```

逻辑回归( logistic regression )的一般数学方程：

$$
\pi(Y=k|X=(X_1,X_2,...,X_p)=\frac{e^{\beta_{k0}+\beta_{k1}X_1+\beta_{k2}X_2+...+\beta_{kp}X_p}}{1+\sum_{l=1}^{K-1} e^{\beta_{l0}+\beta_{l1}X_1+\beta_{l2}X_2+...+\beta_{lp}X_p}}
$$ 其中$\pi$ 是成功概率，$k=1,2,...,K-1$是因变量的水平数，$p$ 是自变量个数。

逻辑回归一般需要引入虚拟变量（哑变量，dummy variable），通常取值伪0或1。

1.  当$K=2$时，$k=l=p=1$即简单逻辑回归。

    极大似然法（maximum likelihood），*likelihood function*：

    $$
    \ell (\beta_0,\beta_1)=\prod_{i:y_i=1}\pi(x_i)\prod_{i':y_{i'}=0}(1-\pi(x_{i'}))
    $$

2.  当$K=2$时，$k=l=1,p>1$即多元逻辑回归（multiple logistic regression）。

    优势（odds）

    $$
    Odds=\frac{\pi(X)}{1-\pi(X)}=e^{\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p}
    $$

    log odds (logit)

    $$
    logit(\pi(X))=\ln (\frac{\pi(X)}{1-\pi(X)})=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
    $$

3.  当$K>2$时，$k,l,p>1$即多项逻辑回归（multinomial logistic regression）。

    $$
    \log (\frac{P(Y=k|X=x)}{P(Y=K|X=x)})=\beta_{k0}+\beta_{k1}X_1+\beta_{k2}X_2+...+\beta_{kp}X_p
    $$

[数据下载网站](https://www.statlearning.com/resources-second-edition)

```{r}
df <- read_csv("data/ISLR/Default.csv")
df$default <- factor(df$default,levels = c("No","Yes"),labels = c(0,1))
df$student<- factor(df$student,levels = c("No","Yes"),labels = c(0,1))
# 违约 学生 余额 收入
head(df)
table(df$default,df$student)
```

```{r}
ggplot(df,aes(balance,income))+
  geom_point(aes(shape=default,color=default),show.legend = F)|
ggplot(df,aes(default,balance,fill=default),)+
  geom_boxplot(show.legend = F)+
ggplot(df,aes(default,income,fill=default))+
  geom_boxplot()
```

### Binary logistic regression

#### 为什么不用线性回归

```{r}
lm_spec<-linear_reg(mode ="regression",engine = "lm" )

lm_default_balance<-lm_spec %>% 
  fit(as.numeric(default)-1~balance,data=df)
lm_default_balance$fit

ggplot(df,aes(balance,as.numeric(default)-1))+
  geom_point(color="orange",size=1.25)+
  geom_smooth(method = "lm",se=FALSE)+
  geom_hline(yintercept = c(0,1),linetype=2)+
  ggtitle("linear regression")

```

#### 逻辑回归

logit link function

z-statistic

```{r}
logit_spec <- logistic_reg() %>%
  set_engine("glm",family= binomial(link = "logit")) 


logit_default_balance <- logit_spec %>% fit(default~balance,data=df)

logit_default_balance %>% glance()

tidy(logit_default_balance)


ggplot(df,aes(balance,as.numeric(default)-1))+
  geom_point(color="orange",size=1.25)+
  geom_smooth(method = "glm",
              method.args=list(family=binomial(link = "logit")),se=FALSE)+
  geom_hline(yintercept = c(0,1),linetype=2)+
  ggtitle("logistic regression")

```

#### 自变量含分类变量

```{r}
logit_default_student <- logit_spec %>%fit(default ~ student, data = df)

tidy(logit_default_student)

df %>% 
  mutate(
    prob=1/(1+exp(-(logit_default_student$fit$coefficients[1]+logit_default_student$fit$coefficients[2]*(as.numeric(student)-1)))),
    logit=log(prob/(1-prob))
  ) %>% 
    dplyr::select(student,prob) %>% 
    DT::datatable()
```

### K=2,p\>1 多元逻辑回归

```{r}
logit_multiple<-logit_spec %>% fit(default~balance+income+student,data=df)

tidy(logit_multiple)

# confusion matrix 混淆矩阵
augment(logit_multiple, new_data = df) %>%
  conf_mat(truth = default, estimate = .pred_class) %>% 
    autoplot(type = "heatmap")

#准确性 
(9627+105)/(9627+105+40+228)
augment(logit_multiple, new_data = df) %>%
  accuracy(truth = default, estimate = .pred_class)
```

减少弱相关或无关变量

```{r}
logit_multiple_2<-logit_spec %>% fit(default~balance+student,data=df)

augment(logit_multiple_2, new_data =df) %>%
  conf_mat(truth = default, estimate = .pred_class) 
```

#### 预测特定值

```{r}
df_new <- tibble(
  balance = c(1000, 2000), 
  student = factor(c(1, 0)),
)
predict(logit_multiple_2, new_data = df_new,type="class")
predict(logit_multiple_2, new_data = df_new, type = "prob")
```

#### 优势比，Wald卡方检验

```{r}
library(broom)

# 示例数据
set.seed(123)
n <- 200
data <- data.frame(
    outcome = sample(c(0, 1), n, replace = TRUE),
    age = rnorm(n, 50, 10),
    gender = factor(sample(c("male", "female"), n, replace = TRUE)),
    bmi = rnorm(n, 25, 5)
)

# 拟合逻辑回归模型
logit_model <- glm(outcome ~ age + gender + bmi, data = data, family = binomial)

logit_model


summary <- summary(logit_model)

# 提取模型结果

model_summary <- tidy(logit_model, conf.int = TRUE)  # 计算置信区间

# 计算优势比（OR）
model_summary <- model_summary %>%
    mutate(
        estimate_OR = exp(estimate),
        # 计算优势比
        conf.low_OR = exp(conf.low),
        # 置信区间下限
        conf.high_OR = exp(conf.high),  # 置信区间上限
        Estimate = summary$coefficients[, "Estimate"],
        Std.Error = summary$coefficients[, "Std. Error"],
        z_value = summary$coefficients[, "Estimate"] / summary$coefficients[, "Std. Error"],
        Wald.ChiSq = (summary$coefficients[, "Estimate"] / summary$coefficients[, "Std. Error"]) ^
            2,
        # Wald卡方值可以用来检验各个变量系数是否显著不同于零。它是通过系数估计的平方除以其标准误差的平方来计算的。
        # 即 z值的平方
        P.Value = summary$coefficients[, "Pr(>|z|)"]
    )


# 打印结果表格
print(model_summary)
```

### 有序逻辑回归

$$
    \log \left(\frac{P(Y\le k|X=x)}{1-P(Y\le k|X=x)}\right)
$$

```{r}
acl <- read_rds("data/icpsr/advanced_acl_data.rds")
acl$PhysActCat_W1 <- factor(acl$PhysActCat_W1,ordered = T)
levels(acl$PhysActCat_W1)

ordered_logit <- MASS::polr(PhysActCat_W1 ~ SelfEfficacy_W1, data = acl,
                            method = "logistic")
ordered_logit %>% summary()


predict(ordered_logit ,acl ,type = "p") %>% 
    as_tibble() %>% 
    DT::datatable()
```

### K\>2,p\>1 多分类逻辑回归

用于处理具有多于两个类别的响应变量的情况。例如，分类问题中的三个或更多类别。

#### `nnet::multinom()`

```{r}
mn_spec <- multinom_reg(mode = "classification", engine = "nnet")

iris_mnlogit <- mn_spec %>% 
    fit(Species ~ ., data = iris)

iris_mnlogit %>% glance()
iris_mnlogit %>% tidy()


augment(iris_mnlogit, new_data = iris) %>%
    conf_mat(truth = Species, estimate = .pred_class) %>%
    autoplot(type = "heatmap")
```

#### `glmnet::glmnet()`

```{r}
library(glmnet) # 多项回归
iris_glmnet <- glmnet(x = iris[, -5], y = iris[, 5], family = "multinomial")
iris_glmnet
summary(iris_glmnet )
plot(iris_glmnet)
plot(iris_glmnet$lambda,
  ylab = expression(lambda), xlab = "迭代次数", main = "惩罚系数的迭代路径"
)

# 选择一个迭代趋于稳定时的 lambda，比如 iris_glmnet$lambda[80]
coef(iris_glmnet, s = 0.0002796185)

iris_pred_glmnet <- predict(
  object = iris_glmnet, newx = as.matrix(iris[, -5]),
  s = 0.0002796185, type = "class"
)

```

```{r}
mn_spec <- multinom_reg(mode = "classification", engine = "glmnet" ,penalty = tune())

iris_mnlogit <- mn_spec %>% 
    fit(Species ~ ., data = iris)
iris_mnlogit %>% glance()
iris_mnlogit$fit %>% tidy() %>% DT::datatable()
```

## 泊松回归

泊松回归用于计数数据，假设响应变量服从泊松分布，并使用对数连接函数（log link function）。z-statistic

`family=poisson(link = "log")`

`family = quasipoisson(link = "log"))`

$$
P(X=x;\lambda)=\frac{e^{-\lambda}\lambda ^x}{x!}
$$

```{r}

ggplot(tibble(x=0:20,
              y1=dpois(x,lambda = 2),
              y2=dpois(x,lambda = 6),
              ),
       aes(x)
       )+
    geom_col(aes(y=y1),fill = "lightblue")+
    geom_col(aes(y=y2),fill = "yellow",alpha=.3)+
    ylab("Poisson Density")


```

```{r}
library(poissonreg)
df2 <- read_csv("data/ISLR/Bikeshare.csv")
```

```{r}
# 泊松回归模型
pois_spec <- poisson_reg() %>% 
  set_mode("regression") %>% 
  set_engine("glm",family=poisson(link = "log"))

pois_rec_spec <- recipe(bikers ~ mnth + hr + workingday + temp + weathersit, data = df2) %>% 
    step_dummy(all_nominal_predictors()) # 虚拟变量

pois_wf <- workflow() %>% 
  add_recipe(pois_rec_spec) %>% 
  add_model(pois_spec)

pois_fit <- pois_wf %>% fit(data = df2)


tidy(pois_fit)


# 绘制实际值与预测值的关系图
augment(pois_fit, new_data = df2, type = "response") %>%
    ggplot(aes(bikers, .pred)) +
    geom_point(alpha = 0.1) +
    geom_abline(slope = 1,
                linewidth = 1,
                color = "grey40") +
    labs(title = "Predicting the number of bikers per hour using Poission Regression", x = "Actual", y = "Predicted")
```

```{r}
pois_fit_coef_mnths <- 
  tidy(pois_fit) %>% 
  dplyr::filter(grepl("^mnth", term)) %>% 
  mutate(
    term = stringr::str_replace(term, "mnth_", ""),
    term = forcats::fct_inorder(term)
  ) 

pois_fit_coef_mnths %>% 
  ggplot(aes(term, estimate)) +
  geom_line(group = 1,na.rm = TRUE) +
  geom_point(shape = 21, size = 3, stroke = 1.5, 
             fill = "black", color = "white",na.rm = TRUE) +
  labs(title = "Coefficient value from Poission Regression",
       x = "Month", y = "Coefficient")
```

```{r}
pois_acl <- pois_spec %>% 
    fit(NChronic12_W1 ~ SelfEfficacy_W1,data = acl)

pois_acl %>% glance()

pois_acl %>% tidy()

AIC(pois_acl$fit)
BIC(pois_acl$fit)
```

## 负二项回归

负二项回归用于处理计数数据且存在过度离散（overdispersion）的问题。

log link function，z-statistic

probability mass function ：

$$
P(X=x;\lambda,\nu)=\binom{x+\nu - 1}{ x} \left ( \frac{\lambda}{\lambda +\nu} \right)^x \left ( \frac{\nu}{\nu + \lambda} \right)^{\nu}
$$

负二项分布的均值是 $\lambda$ ，

方差是 $\lambda + \frac{\lambda ^2}{\nu}$ 。

```{r}
library(MASS)
# 负二项回归模型
nb_spec <- linear_reg() %>% 
  set_engine("glm", family = MASS::negative.binomial(theta = 1, link = "log"))

nb_acl <- nb_spec %>% 
  fit(NChronic12_W1 ~ SelfEfficacy_W1, data = acl)

# 查看模型结果
nb_acl %>% glance()
nb_acl %>% tidy()


AIC(nb_acl$fit)
BIC(nb_acl$fit)

# MASS::glm.nb()
```
