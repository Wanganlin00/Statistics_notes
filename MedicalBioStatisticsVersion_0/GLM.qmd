# 广义线性模型

## 广义线性模型组件

广义线性模型是对线性模型的扩展，适用于非正态分布的数据，假设观测值之间是独立的，不能处理组内相关性。模型形式为：

$$
g(E(Y))=\mathbf{X} \beta
$$

1.  线性预测函数：

$$
\eta = \mathbf{X} \beta
$$

2.  因变量的期望值与线性预测函数的关系：

$$
E(y)=\mu
$$

3.  连接函数 `g(.)` ：

$$
\eta =g(\mu)=g(E(y))
$$

4.  反连接函数`g-1 (.)`：

$$
E(y)=g^{-1}(\eta)
$$

```{r}
library(tidymodels)
library(poissonreg)
library(patchwork)
```

## 高斯线性回归

高斯线性回归是一种简单的线性回归模型，其中假设响应变量服从正态分布，并且使用恒等连接函数（identity link function）。t-statistic

```{r}
library(tidymodels)

# 使用 glm() 函数进行高斯线性回归
glm1 <- linear_reg() |> 
  set_engine("glm", family = stats::gaussian(link = "identity")) |> 
  fit(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality,
      data = swiss)

# 查看模型的系数
tidy(glm1)

# 查看模型的 AIC 和 Deviance
glance(glm1) |> select(AIC, deviance)

```

## 逻辑回归

逻辑回归用于处理二元分类问题。其模型假设响应变量的对数优势（log odds）服从线性模型。

Sigmoid 激活函数：

$$
f(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{1+e^x}
$$

```{r}
sigmoid <- tibble(
    x=seq(-6,6,length.out=1000),
    y=1/(1+exp(-x)),
)
ggplot(sigmoid,aes(x,y))+
    geom_line()
```

逻辑回归( logistic regression )的一般数学方程：

$$
\pi(Y=k|X=(X_1,X_2,...,X_p)=\frac{e^{\beta_{k0}+\beta_{k1}X_1+\beta_{k2}X_2+...+\beta_{kp}X_p}}{1+\sum_{l=1}^{K-1} e^{\beta_{l0}+\beta_{l1}X_1+\beta_{l2}X_2+...+\beta_{lp}X_p}}
$$ 其中$\pi$ 是成功概率，$k=1,2,...,K-1$是因变量的水平数，$p$ 是自变量个数。

逻辑回归一般需要引入虚拟变量（哑变量，dummy variable），通常取值伪0或1。

1.  当$K=2$时，$k=l=p=1$即简单逻辑回归。

    极大似然法（maximum likelihood），*likelihood function*：

    $$
    \ell (\beta_0,\beta_1)=\prod_{i:y_i=1}\pi(x_i)\prod_{i':y_{i'}=0}(1-\pi(x_{i'}))
    $$

2.  当$K=2$时，$k=l=1,p>1$即多元逻辑回归（multiple logistic regression）。

    优势（odds）

    $$
    Odds=\frac{\pi(X)}{1-\pi(X)}=e^{\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p}
    $$

    log odds (logit)

    $$
    logit(\pi(X))=\ln (\frac{\pi(X)}{1-\pi(X)})=\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p
    $$

3.  当$K>2$时，$k,l,p>1$即多项逻辑回归（multinomial logistic regression）。

    $$
    \log (\frac{P(Y=k|X=x)}{P(Y=K|X=x)})=\beta_{k0}+\beta_{k1}X_1+\beta_{k2}X_2+...+\beta_{kp}X_p
    $$

[数据下载网站](https://www.statlearning.com/resources-second-edition)

```{r}
df <- read_csv("data/ISLR/Default.csv")
df$default <- factor(df$default,levels = c("No","Yes"),labels = c(0,1))
df$student<- factor(df$student,levels = c("No","Yes"),labels = c(0,1))
# 违约 学生 余额 收入
head(df)
table(df$default,df$student)
```

```{r}
ggplot(df,aes(balance,income))+
  geom_point(aes(shape=default,color=default),show.legend = F)|
ggplot(df,aes(default,balance,fill=default),)+
  geom_boxplot(show.legend = F)+
ggplot(df,aes(default,income,fill=default))+
  geom_boxplot()
```

### Binary logistic regression

#### 为什么不用线性回归

```{r}
lm_spec<-linear_reg(mode ="regression",engine = "lm" )

lm_default_balance<-lm_spec |> 
  fit(as.numeric(default)-1~balance,data=df)
lm_default_balance$fit

ggplot(df,aes(balance,as.numeric(default)-1))+
  geom_point(color="orange",size=1.25)+
  geom_smooth(method = "lm",se=FALSE)+
  geom_hline(yintercept = c(0,1),linetype=2)+
  ggtitle("linear regression")

```

#### 逻辑回归

logit link function

z-statistic

```{r}
logit_spec <- logistic_reg() |>
  set_engine("glm",family= binomial(link = "logit")) 


logit_default_balance <- logit_spec |> fit(default~balance,data=df)

logit_default_balance |> glance()

tidy(logit_default_balance)


ggplot(df,aes(balance,as.numeric(default)-1))+
  geom_point(color="orange",size=1.25)+
  geom_smooth(method = "glm",
              method.args=list(family=binomial(link = "logit")),se=FALSE)+
  geom_hline(yintercept = c(0,1),linetype=2)+
  ggtitle("logistic regression")

```

#### 自变量是分类变量

```{r}
logit_default_student <- logit_spec |>fit(default ~ student, data = df)

tidy(logit_default_student)

df |> 
  mutate(
    prob=1/(1+exp(-(logit_default_student$fit$coefficients[1]+logit_default_student$fit$coefficients[2]*(as.numeric(student)-1)))),
    logit=log(prob/(1-prob))
  ) |> select(student,prob)
```

### K=2,p\>1 多元逻辑回归

```{r}
logit_multiple<-logit_spec |> fit(default~balance+income+student,data=df)

tidy(logit_multiple)

# confusion matrix 混淆矩阵
augment(logit_multiple, new_data = df) |>
  conf_mat(truth = default, estimate = .pred_class) |> 
    autoplot(type = "heatmap")

#准确性 
(9627+105)/(9627+105+40+228)
augment(logit_multiple, new_data = df) |>
  accuracy(truth = default, estimate = .pred_class)
```

减少弱相关或无关变量

```{r}
logit_multiple_2<-logit_spec |> fit(default~balance+student,data=df)

augment(logit_multiple_2, new_data =df) |>
  conf_mat(truth = default, estimate = .pred_class) 
```

预测特定值

```{r}
df_new <- tibble(
  balance = c(1000, 2000), 
  student = factor(c(1, 0)),
)
predict(logit_multiple_2, new_data = df_new,type="class")
predict(logit_multiple_2, new_data = df_new, type = "prob")
```

### 有序逻辑回归

$$
    \log \left(\frac{P(Y\le k|X=x)}{1-P(Y\le k|X=x)}\right)
$$

```{r}
acl <- read_rds("data/icpsr/advanced_acl_data.rds")
acl$PhysActCat_W1 <- factor(acl$PhysActCat_W1,ordered = T)
levels(acl$PhysActCat_W1)

ordered_logit <- MASS::polr(PhysActCat_W1 ~ SelfEfficacy_W1, data = acl,
                            method = "logistic")
ordered_logit |> summary()


predict(ordered_logit ,acl ,type = "p") |> as_tibble()
```

### K\>2,p\>1 多分类逻辑回归

用于处理具有多于两个类别的响应变量的情况。例如，分类问题中的三个或更多类别。

#### `nnet::multinom()`

```{r}
mn_spec <- multinom_reg(mode = "classification", engine = "nnet")

iris_mnlogit <- mn_spec |> 
    fit(Species ~ ., data = iris)

iris_mnlogit |> glance()
iris_mnlogit |> tidy()


augment(iris_mnlogit, new_data = iris) |>
    conf_mat(truth = Species, estimate = .pred_class) |>
    autoplot(type = "heatmap")
```

#### `glmnet::glmnet()`

```{r}
library(glmnet) # 多项回归
iris_glmnet <- glmnet(x = iris[, -5], y = iris[, 5], family = "multinomial")
iris_glmnet
summary(iris_glmnet )
plot(iris_glmnet)
plot(iris_glmnet$lambda,
  ylab = expression(lambda), xlab = "迭代次数", main = "惩罚系数的迭代路径"
)

# 选择一个迭代趋于稳定时的 lambda，比如 iris_glmnet$lambda[80]
coef(iris_glmnet, s = 0.0002796185)

iris_pred_glmnet <- predict(
  object = iris_glmnet, newx = as.matrix(iris[, -5]),
  s = 0.0002796185, type = "class"
)

```

```{r}
mn_spec <- multinom_reg(mode = "classification", engine = "glmnet" ,penalty = tune())

iris_mnlogit <- mn_spec |> 
    fit(Species ~ ., data = iris)
iris_mnlogit |> glance()
iris_mnlogit$fit
```

## 泊松回归

泊松回归用于计数数据，假设响应变量服从泊松分布，并使用对数连接函数（log link function）。z-statistic

`family=poisson(link = "log")`

`family = quasipoisson(link = "log"))`

$$
P(X=x;\lambda)=\frac{e^{-\lambda}\lambda ^x}{x!}
$$

```{r}

ggplot(tibble(x=0:20,
              y1=dpois(x,lambda = 2),
              y2=dpois(x,lambda = 6),
              ),
       aes(x)
       )+
    geom_col(aes(y=y1),fill = "lightblue")+
    geom_col(aes(y=y2),fill = "yellow",alpha=.3)+
    ylab("Poisson Density")


```

```{r}
library(poissonreg)
df2 <- read_csv("data/ISLR/Bikeshare.csv")
```

```{r}
# 泊松回归模型
pois_spec <- poisson_reg() |> 
  set_mode("regression") |> 
  set_engine("glm",family=poisson(link = "log"))

pois_rec_spec <- recipe(bikers ~ mnth + hr + workingday + temp + weathersit, data = df2) |> 
    step_dummy(all_nominal_predictors()) # 虚拟变量

pois_wf <- workflow() |> 
  add_recipe(pois_rec_spec) |> 
  add_model(pois_spec)

pois_fit <- pois_wf |> fit(data = df2)


tidy(pois_fit)


# 绘制实际值与预测值的关系图
augment(pois_fit, new_data = df2,type="response") |> 
  ggplot(aes(bikers, .pred)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, linewidth = 1, color = "grey40") +
  labs(title = "Predicting the number of bikers per hour using Poission Regression",
       x = "Actual", y = "Predicted")
```

```{r}
pois_fit_coef_mnths <- 
  tidy(pois_fit) |> 
  dplyr::filter(grepl("^mnth", term)) |> 
  mutate(
    term = stringr::str_replace(term, "mnth_", ""),
    term = forcats::fct_inorder(term)
  ) 

pois_fit_coef_mnths |> 
  ggplot(aes(term, estimate)) +
  geom_line(group = 1,na.rm = TRUE) +
  geom_point(shape = 21, size = 3, stroke = 1.5, 
             fill = "black", color = "white",na.rm = TRUE) +
  labs(title = "Coefficient value from Poission Regression",
       x = "Month", y = "Coefficient")
```

```{r}
pois_acl <- pois_spec |> 
    fit(NChronic12_W1 ~ SelfEfficacy_W1,data = acl)

pois_acl |> glance()

pois_acl |> tidy()

AIC(pois_acl$fit)
BIC(pois_acl$fit)
```

## 负二项回归

负二项回归用于处理计数数据且存在过度离散（overdispersion）的问题。

log link function，z-statistic

probability mass function ：

$$
P(X=x;\lambda,\nu)=\binom{x+\nu - 1}{ x} \left ( \frac{\lambda}{\lambda +\nu} \right)^x \left ( \frac{\nu}{\nu + \lambda} \right)^{\nu}
$$

负二项分布的均值是 $\lambda$ ，

方差是 $\lambda + \frac{\lambda ^2}{\nu}$ 。

```{r}
library(MASS)
# 负二项回归模型
nb_spec <- linear_reg() |> 
  set_engine("glm", family = MASS::negative.binomial(theta = 1, link = "log"))

nb_acl <- nb_spec |> 
  fit(NChronic12_W1 ~ SelfEfficacy_W1, data = acl)

# 查看模型结果
nb_acl |> glance()
nb_acl |> tidy()


AIC(nb_acl$fit)
BIC(nb_acl$fit)

# MASS::glm.nb()
```
